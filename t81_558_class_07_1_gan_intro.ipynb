{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj6p0QRN4JOD"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_1_gan_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UL-Y_bXE4JOD"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 7: Generative Adversarial Networks**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2m8cZup4JOE"
   },
   "source": [
    "# Module 7 Material\n",
    "\n",
    "* **Part 7.1: Introduction to GANs for Image and Data Generation** [[Video]](https://www.youtube.com/watch?v=hZw-AjbdN5k&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_1_gan_intro.ipynb)\n",
    "* Part 7.2: Train StyleGAN3 with your Own Images [[Video]](https://www.youtube.com/watch?v=R546LYsQk5M&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_train_gan.ipynb)\n",
    "* Part 7.3: Exploring the StyleGAN Latent Vector [[Video]](https://www.youtube.com/watch?v=goQzp8QSb2s&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_3_latent_vector.ipynb)\n",
    "* Part 7.4: GANs to Enhance Old Photographs Deoldify [[Video]](https://www.youtube.com/watch?v=0OTd5GlHRx4&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_4_deoldify.ipynb)\n",
    "* Part 7.5: GANs for Tabular Synthetic Data Generation [[Video]](https://www.youtube.com/watch?v=yujdA46HKwA&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_5_tabular_synthetic.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOcBZtt84rrd"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrshAzVK4smS",
    "outputId": "7e0743cc-f46e-4c67-e61f-186705b36a2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    %tensorflow_version 2.x\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avFNBe3j4JOE"
   },
   "source": [
    "# Part 7.1: Introduction to GANS for Image and Data Generation\n",
    "\n",
    "A generative adversarial network (GAN) is a class of machine learning systems invented by Ian Goodfellow in 2014. [[Cite:goodfellow2014generative]](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) Two neural networks compete with each other in a game. **The GAN training algorithm starts with a training set and learns to generate new data with the same distributions as the training set.** For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. \n",
    "\n",
    "This chapter makes use of the PyTorch framework rather than Keras/TensorFlow. While there are versions of [StyleGAN2-ADA that work with TensorFlow 1.0](https://github.com/jeffheaton/t81_558_deep_learning/blob/5e2528a08c302c82919001a3c3c8364c29c1b999/t81_558_class_07_3_style_gan.ipynb), NVIDIA has switched to PyTorch for StyleGAN. Running this notebook in Google CoLab is the most straightforward means of completing this chapter. Because of this, I designed this notebook to run in Google CoLab. It will take some modifications if you wish to run it locally.\n",
    "\n",
    "This original StyleGAN paper used neural networks to automatically generate images for several previously seen datasets: MINST and CIFAR. However, it also included the Toronto Face Dataset (a private dataset used by some researchers). You can see some of these images in Figure 7.GANS.\n",
    "\n",
    "**Figure 7.GANS: GAN Generated Images**\n",
    "![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan-2.png \"GAN Generated Images\")\n",
    "\n",
    "Only sub-figure D made use of convolutional neural networks. Figures A-C make use of fully connected neural networks. As we will see in this module, the researchers significantly increased the role of convolutional neural networks for GANs.\n",
    "\n",
    "We call a GAN a generative model because it generates new data. You can see the overall process in Figure 7.GAN-FLOW.\n",
    "\n",
    "**Figure 7.GAN-FLOW: GAN Structure**\n",
    "![GAN Structure](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan-1.png \"GAN Structure\")\n",
    "\n",
    "## Face Generation with StyleGAN and Python\n",
    "\n",
    "GANs have appeared frequently in the media, showcasing their ability to generate highly photorealistic faces. One significant step forward for realistic face generation was the NVIDIA StyleGAN series. NVIDIA introduced the origional StyleGAN in 2018. [[Cite:karras2019style]](https://arxiv.org/abs/1812.04948) StyleGAN was followed by StyleGAN2 in 2019, which improved the quality of StyleGAN by removing certian artifacts. [[Cite:karras2019analyzing]](https://arxiv.org/abs/1912.04958) Most recently, in 2020, NVIDIA released StyleGAN2 adaptive discriminator augmentation (ADA), which will be the focus of this module. [[Cite:karras2020training]](https://arxiv.org/abs/2006.06676)  We will see both how to train StyleGAN2 ADA on any arbitray set of images; as well as use pretrained weights provided by NVIDIA. The NVIDIA weights allow us to generate high resolution photorealistic looking faces, such seen in Figure 7.STY-GAN.\n",
    "\n",
    "**Figure 7.STY-GAN: StyleGAN2 Generated Faces**\n",
    "![StyleGAN2 Generated Faces](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/stylegan2_images.jpg \"StyleGAN2 Generated Faces\")\n",
    "\n",
    "The above images were generated with StyleGAN2, using Google CoLab. Following the instructions in this section, you will be able to create faces like this of your own. StyleGAN2 images are usually 1,024 x 1,024 in resolution.  An example of a full-resolution StyleGAN image can be [found here](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/stylegan2-hires.jpg). \n",
    "\n",
    "The primary advancement introduced by the adaptive discriminator augmentation is that the algorithm augments the training images in real-time. Image augmentation is a common technique in many convolution neural network applications. Augmentation has the effect of increasing the size of the training set. Where StyleGAN2 previously required over 30K images for an effective to develop an effective neural network; now much fewer are needed. I used 2K images to train the fish generating GAN for this section. Figure 7.STY-GAN-ADA demonstrates the ADA process.\n",
    "\n",
    "**Figure 7.STY-GAN-ADA: StyleGAN2 ADA Training**\n",
    "![StyleGAN2 Generated Faces](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/stylegan2-ada-teaser-1024x252.jpg \"StyleGAN2 Generated Faces\")\n",
    "\n",
    "The figure shows the increasing probability of augmentation as $p$ increases. For small image sets, the discriminator will generally memorize the image set unless the training algorithm makes use of augmentation. Once this memorization occurs, the discriminator is no longer providing useful information to the training of the generator.\n",
    "\n",
    "While the above images look much more realistic than images generated earlier in this course, they are not perfect. Look at Figure 7.STYLEGAN2. There are usually several tell-tail signs that you are looking at a computer-generated image. One of the most obvious is usually the surreal, dream-like backgrounds. The background does not look obviously fake at first glance; however, upon closer inspection, you usually can't quite discern what a GAN-generated background is. Also, look at the image character's left eye. It is slightly unrealistic looking, especially near the eyelashes.\n",
    "\n",
    "Look at the following GAN face. Can you spot any imperfections?\n",
    "\n",
    "**Figure 7.STYLEGAN2: StyleGAN2 Face**\n",
    "![StyleGAN2 Face](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan_bad.jpg \"StyleGAN2 Face\")\n",
    "\n",
    "* Image A demonstrates the abstract backgrounds usually associated with a GAN-generated image.\n",
    "* Image B exhibits issues that earrings often present for GANs. GANs sometimes have problems with symmetry, particularly earrings.\n",
    "* Image C contains an abstract background and a highly distorted secondary image.\n",
    "* Image D also contains a highly distorted secondary image that might be a hand.\n",
    "\n",
    "Several websites allow you to generate GANs of your own without any software.\n",
    "\n",
    "* [This Person Does not Exist](https://www.thispersondoesnotexist.com/)\n",
    "* [Which Face is Real](http://www.whichfaceisreal.com/)\n",
    "\n",
    "The first site generates high-resolution images of human faces. The second site presents a quiz to see if you can detect the difference between a real and fake human face image.\n",
    "\n",
    "In this chapter, you will learn to create your own StyleGAN pictures using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rq3dZOg_5GNH"
   },
   "source": [
    "## Generating High Rez GAN Faces with Google CoLab\n",
    "\n",
    "This notebook demonstrates how to run [NVidia StyleGAN2 ADA](https://github.com/NVlabs/stylegan2-ada) inside a Google CoLab notebook.  I suggest you use this to generate GAN faces from a pretrained model.  If you try to train your own, you will run into compute limitations of Google CoLab. Make sure to run this code on a GPU instance.  GPU is assumed.\n",
    "\n",
    "First, we clone StyleGAN3 from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/adeng/Py_workingdir/ml_learning/t81_558_deep_learning'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjXaut8E4JOE",
    "outputId": "26a7e12f-87c0-448f-b0e9-706262976f4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'stylegan3' already exists and is not an empty directory.\n",
      "Requirement already satisfied: ninja in /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages (1.11.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# HIDE OUTPUT\n",
    "!git clone https://github.com/NVlabs/stylegan3.git\n",
    "%pip install ninja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAWeiW2A5Wub"
   },
   "source": [
    "\n",
    "Verify that StyleGAN has been cloned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZF5jtAT4JOF",
    "outputId": "165519bf-39ea-46be-a0d7-82a9a558e0c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_spectra.py\t Dockerfile\t  gen_video.py\tmetrics      training\n",
      "calc_metrics.py  docs\t\t  gui_utils\t__pycache__  train.py\n",
      "dataset_tool.py  environment.yml  legacy.py\tREADME.md    visualizer.py\n",
      "dnnlib\t\t gen_images.py\t  LICENSE.txt\ttorch_utils  viz\n"
     ]
    }
   ],
   "source": [
    "!ls ./stylegan3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17md6jij5bPv"
   },
   "source": [
    "## Run StyleGan From Command Line [not successful due to config issues]\n",
    "\n",
    "Add the StyleGAN folder to Python so that you can import it. I based this code below on code from NVidia for the original StyleGAN paper. When you use StyleGAN you will generally create a GAN from a seed number. This seed is an integer, such as 6600, that will generate a unique image. The seed generates a latent vector containing 512 floating-point values. The GAN code uses the seed to generate these 512 values. The seed value is easier to represent in code than a 512 value vector; however, while a small change to the latent vector results in a slight change to the image, even a small change to the integer seed value will produce a radically different image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adeng/miniconda3/envs/torch/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUA_V0yH5f4Y",
    "outputId": "3b819900-b396-4197-de85-e8f216440182",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading networks from \"{URL}\"...\n",
      "Traceback (most recent call last):\n",
      "  File \"./stylegan3/gen_images.py\", line 143, in <module>\n",
      "    generate_images() # pylint: disable=no-value-for-parameter\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/click/core.py\", line 1128, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/click/core.py\", line 1053, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/click/core.py\", line 1395, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/click/core.py\", line 754, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"./stylegan3/gen_images.py\", line 107, in generate_images\n",
      "    with dnnlib.util.open_url(network_pkl) as f:\n",
      "  File \"/home/adeng/Py_workingdir/ml_learning/t81_558_deep_learning/stylegan3/dnnlib/util.py\", line 403, in open_url\n",
      "    return url if return_filename else open(url, \"rb\")\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '{URL}'\n"
     ]
    }
   ],
   "source": [
    "# HIDE OUTPUT\n",
    "URL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/\"\\\n",
    "      \"stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl\"\n",
    "\n",
    "# set CUDA_HOME env var and run the script\n",
    "\n",
    "!export CUDA_HOME=$CONDA_PREFIX; \\\n",
    "/home/adeng/miniconda3/envs/torch/bin/python ./stylegan3/gen_images.py \\\n",
    "  --network={URL} \\\n",
    "  --outdir=./results --seeds=6600-6625 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading networks from \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl\"...\n",
      "Generating image for seed 6600 (0/26) ...\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... Failed!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/cpp_extension.py\", line 1906, in _run_ninja_build\n",
      "    env=env)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/subprocess.py\", line 512, in run\n",
      "    output=stdout, stderr=stderr)\n",
      "subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"./stylegan3/gen_images.py\", line 143, in <module>\n",
      "    generate_images() # pylint: disable=no-value-for-parameter\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/click/core.py\", line 1128, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/click/core.py\", line 1053, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/click/core.py\", line 1395, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/click/core.py\", line 754, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"./stylegan3/gen_images.py\", line 135, in generate_images\n",
      "    img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<string>\", line 503, in forward\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<string>\", line 143, in forward\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<string>\", line 92, in forward\n",
      "  File \"/home/adeng/Py_workingdir/ml_learning/t81_558_deep_learning/stylegan3/torch_utils/ops/bias_act.py\", line 84, in bias_act\n",
      "    if impl == 'cuda' and x.device.type == 'cuda' and _init():\n",
      "  File \"/home/adeng/Py_workingdir/ml_learning/t81_558_deep_learning/stylegan3/torch_utils/ops/bias_act.py\", line 46, in _init\n",
      "    extra_cuda_cflags=['--use_fast_math', '--allow-unsupported-compiler'],\n",
      "  File \"/home/adeng/Py_workingdir/ml_learning/t81_558_deep_learning/stylegan3/torch_utils/custom_ops.py\", line 137, in get_plugin\n",
      "    verbose=verbose_build, sources=cached_sources, **build_kwargs)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/cpp_extension.py\", line 1296, in load\n",
      "    keep_intermediates=keep_intermediates)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/cpp_extension.py\", line 1518, in _jit_compile\n",
      "    is_standalone=is_standalone)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/cpp_extension.py\", line 1626, in _write_ninja_file_and_build_library\n",
      "    error_prefix=f\"Error building extension '{name}'\")\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/cpp_extension.py\", line 1916, in _run_ninja_build\n",
      "    raise RuntimeError(message) from e\n",
      "RuntimeError: Error building extension 'bias_act_plugin': [1/2] /home/adeng/miniconda3/envs/torch/bin/nvcc  -DTORCH_EXTENSION_NAME=bias_act_plugin -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/TH -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/THC -isystem /home/adeng/miniconda3/envs/torch/include -isystem /home/adeng/miniconda3/envs/torch/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 --compiler-options '-fPIC' --use_fast_math --allow-unsupported-compiler -std=c++14 -c /home/adeng/.cache/torch_extensions/py37_cu117/bias_act_plugin/3cb576a0039689487cfba59279dd6d46-nvidia-geforce-gtx-1080-ti/bias_act.cu -o bias_act.cuda.o \n",
      "FAILED: bias_act.cuda.o \n",
      "/home/adeng/miniconda3/envs/torch/bin/nvcc  -DTORCH_EXTENSION_NAME=bias_act_plugin -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/TH -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/THC -isystem /home/adeng/miniconda3/envs/torch/include -isystem /home/adeng/miniconda3/envs/torch/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 --compiler-options '-fPIC' --use_fast_math --allow-unsupported-compiler -std=c++14 -c /home/adeng/.cache/torch_extensions/py37_cu117/bias_act_plugin/3cb576a0039689487cfba59279dd6d46-nvidia-geforce-gtx-1080-ti/bias_act.cu -o bias_act.cuda.o \n",
      "/usr/include/c++/11/bits/std_function.h:435:145: error: parameter packs not expanded with ‘...’:\n",
      "  435 |         function(_Functor&& __f)\n",
      "      |                                                                                                                                                 ^ \n",
      "/usr/include/c++/11/bits/std_function.h:435:145: note:         ‘_ArgTypes’\n",
      "/usr/include/c++/11/bits/std_function.h:530:146: error: parameter packs not expanded with ‘...’:\n",
      "  530 |         operator=(_Functor&& __f)\n",
      "      |                                                                                                                                                  ^ \n",
      "/usr/include/c++/11/bits/std_function.h:530:146: note:         ‘_ArgTypes’\n",
      "ninja: build stopped: subcommand failed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HIDE OUTPUT\n",
    "URL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/\"\\\n",
    "      \"stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl\"\n",
    "\n",
    "# set CUDA_HOME env var and run the script\n",
    "!python ./stylegan3/gen_images.py \\\n",
    "  --network={URL} \\\n",
    "  --outdir=./results --seeds=6600-6625 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading networks from \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl\"...\n",
      "Generating image for seed 6600 (0/26) ...\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... Failed!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/cpp_extension.py\", line 1906, in _run_ninja_build\n",
      "    env=env)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/subprocess.py\", line 512, in run\n",
      "    output=stdout, stderr=stderr)\n",
      "subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"./stylegan3/gen_images.py\", line 143, in <module>\n",
      "    generate_images() # pylint: disable=no-value-for-parameter\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/click/core.py\", line 1128, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/click/core.py\", line 1053, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/click/core.py\", line 1395, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/click/core.py\", line 754, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"./stylegan3/gen_images.py\", line 135, in generate_images\n",
      "    img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<string>\", line 503, in forward\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<string>\", line 143, in forward\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<string>\", line 92, in forward\n",
      "  File \"/home/adeng/Py_workingdir/ml_learning/t81_558_deep_learning/stylegan3/torch_utils/ops/bias_act.py\", line 84, in bias_act\n",
      "    if impl == 'cuda' and x.device.type == 'cuda' and _init():\n",
      "  File \"/home/adeng/Py_workingdir/ml_learning/t81_558_deep_learning/stylegan3/torch_utils/ops/bias_act.py\", line 46, in _init\n",
      "    extra_cuda_cflags=['--use_fast_math', '--allow-unsupported-compiler'],\n",
      "  File \"/home/adeng/Py_workingdir/ml_learning/t81_558_deep_learning/stylegan3/torch_utils/custom_ops.py\", line 137, in get_plugin\n",
      "    verbose=verbose_build, sources=cached_sources, **build_kwargs)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/cpp_extension.py\", line 1296, in load\n",
      "    keep_intermediates=keep_intermediates)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/cpp_extension.py\", line 1518, in _jit_compile\n",
      "    is_standalone=is_standalone)\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/cpp_extension.py\", line 1626, in _write_ninja_file_and_build_library\n",
      "    error_prefix=f\"Error building extension '{name}'\")\n",
      "  File \"/home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/cpp_extension.py\", line 1916, in _run_ninja_build\n",
      "    raise RuntimeError(message) from e\n",
      "RuntimeError: Error building extension 'bias_act_plugin': [1/3] /home/adeng/miniconda3/envs/torch/bin/nvcc  -DTORCH_EXTENSION_NAME=bias_act_plugin -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/TH -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/THC -isystem /home/adeng/miniconda3/envs/torch/include -isystem /home/adeng/miniconda3/envs/torch/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 --compiler-options '-fPIC' --use_fast_math --allow-unsupported-compiler -std=c++14 -c /home/adeng/.cache/torch_extensions/py37_cu117/bias_act_plugin/3cb576a0039689487cfba59279dd6d46-nvidia-geforce-gtx-1080-ti/bias_act.cu -o bias_act.cuda.o \n",
      "FAILED: bias_act.cuda.o \n",
      "/home/adeng/miniconda3/envs/torch/bin/nvcc  -DTORCH_EXTENSION_NAME=bias_act_plugin -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/TH -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/THC -isystem /home/adeng/miniconda3/envs/torch/include -isystem /home/adeng/miniconda3/envs/torch/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 --compiler-options '-fPIC' --use_fast_math --allow-unsupported-compiler -std=c++14 -c /home/adeng/.cache/torch_extensions/py37_cu117/bias_act_plugin/3cb576a0039689487cfba59279dd6d46-nvidia-geforce-gtx-1080-ti/bias_act.cu -o bias_act.cuda.o \n",
      "/bin/sh: 1: /home/adeng/miniconda3/envs/torch/bin/nvcc: not found\n",
      "[2/3] c++ -MMD -MF bias_act.o.d -DTORCH_EXTENSION_NAME=bias_act_plugin -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/TH -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/THC -isystem /home/adeng/miniconda3/envs/torch/include -isystem /home/adeng/miniconda3/envs/torch/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /home/adeng/.cache/torch_extensions/py37_cu117/bias_act_plugin/3cb576a0039689487cfba59279dd6d46-nvidia-geforce-gtx-1080-ti/bias_act.cpp -o bias_act.o \n",
      "ninja: build stopped: subcommand failed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# set CUDA_HOME env var and run the script\n",
    "\n",
    "!export CUDA_HOME=$CONDA_PREFIX; \\\n",
    "/home/adeng/miniconda3/envs/torch/bin/python ./stylegan3/gen_images.py \\\n",
    "  --network=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl \\\n",
    "  --outdir=./results --seeds=6600-6625 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5eLRW1mGN-G"
   },
   "source": [
    "We can now display the images created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PVRTbk5S5mPI",
    "outputId": "a060e4d6-0be7-48bc-d572-08549fc918f4"
   },
   "outputs": [],
   "source": [
    "!ls ./results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OPbrrs85jXO"
   },
   "source": [
    "Next, copy the images to a folder of your choice on GDrive."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "HLBt0hmv5poU"
   },
   "source": [
    "!cp /content/results/* \\\n",
    "    /content/drive/My\\ Drive/projects/stylegan3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rGCxzWG5uBi"
   },
   "source": [
    "## Run StyleGAN From Python Code\n",
    "\n",
    "Add the StyleGAN folder to Python so that you can import it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NDscPcmE5zbs"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"./stylegan3\")\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "import torch\n",
    "import dnnlib\n",
    "import legacy\n",
    "\n",
    "def seed2vec(G, seed):\n",
    "  return np.random.RandomState(seed).randn(1, G.z_dim)\n",
    "\n",
    "def display_image(image):\n",
    "  plt.axis('off')\n",
    "  plt.imshow(image)\n",
    "  plt.show()\n",
    "\n",
    "def generate_image(G, z, truncation_psi):\n",
    "    # Render images for dlatents initialized from random seeds.\n",
    "    Gs_kwargs = {\n",
    "        'output_transform': dict(func=tflib.convert_images_to_uint8, \n",
    "         nchw_to_nhwc=True),\n",
    "        'randomize_noise': False\n",
    "    }\n",
    "    if truncation_psi is not None:\n",
    "        Gs_kwargs['truncation_psi'] = truncation_psi\n",
    "\n",
    "    label = np.zeros([1] + G.input_shapes[1][1:])\n",
    "    # [minibatch, height, width, channel]\n",
    "    images = G.run(z, label, **G_kwargs) \n",
    "    return images[0]\n",
    "\n",
    "def get_label(G, device, class_idx):\n",
    "  label = torch.zeros([1, G.c_dim], device=device)\n",
    "  if G.c_dim != 0:\n",
    "      if class_idx is None:\n",
    "          ctx.fail(\"Must specify class label with --class when using \"\\\n",
    "            \"a conditional network\")\n",
    "      label[:, class_idx] = 1\n",
    "  else:\n",
    "      if class_idx is not None:\n",
    "          print (\"warn: --class=lbl ignored when running on \"\\\n",
    "            \"an unconditional network\")\n",
    "  return label\n",
    "\n",
    "def generate_image(device, G, z, truncation_psi=1.0, noise_mode='const', \n",
    "                   class_idx=None):\n",
    "  z = torch.from_numpy(z).to(device)\n",
    "  label = get_label(G, device, class_idx)\n",
    "  img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
    "  img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(\\\n",
    "      torch.uint8)\n",
    "  return PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWPdx2PX5567",
    "outputId": "9b9eeb27-704a-4b6a-b829-3de88c7094a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading networks from \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl\"...\n"
     ]
    }
   ],
   "source": [
    "#URL = \"https://github.com/jeffheaton/pretrained-gan-fish/releases/\"\\\n",
    "#  \"download/1.0.0/fish-gan-2020-12-09.pkl\"\n",
    "#URL = \"https://github.com/jeffheaton/pretrained-merry-gan-mas/releases/\"\\\n",
    "#  \"download/v1/christmas-gan-2020-12-03.pkl\"\n",
    "URL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/\"\\\n",
    "  \"versions/1/files/stylegan3-r-ffhq-1024x1024.pkl\"\n",
    "\n",
    "print(f'Loading networks from \"{URL}\"...')\n",
    "device = torch.device('cuda')\n",
    "with dnnlib.util.open_url(URL) as f:\n",
    "    G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate images from integer seed codes in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "id": "4QVzyLIj582S",
    "outputId": "86edbf03-68ea-49a5-9a4d-d548483cc05b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1000\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... Failed!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error building extension 'bias_act_plugin': [1/2] /home/adeng/miniconda3/envs/torch/bin/nvcc  -DTORCH_EXTENSION_NAME=bias_act_plugin -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/TH -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/THC -isystem /home/adeng/miniconda3/envs/torch/include -isystem /home/adeng/miniconda3/envs/torch/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 --compiler-options '-fPIC' --use_fast_math --allow-unsupported-compiler -std=c++14 -c /home/adeng/.cache/torch_extensions/py37_cu117/bias_act_plugin/3cb576a0039689487cfba59279dd6d46-nvidia-geforce-gtx-1080-ti/bias_act.cu -o bias_act.cuda.o \nFAILED: bias_act.cuda.o \n/home/adeng/miniconda3/envs/torch/bin/nvcc  -DTORCH_EXTENSION_NAME=bias_act_plugin -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/TH -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/THC -isystem /home/adeng/miniconda3/envs/torch/include -isystem /home/adeng/miniconda3/envs/torch/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 --compiler-options '-fPIC' --use_fast_math --allow-unsupported-compiler -std=c++14 -c /home/adeng/.cache/torch_extensions/py37_cu117/bias_act_plugin/3cb576a0039689487cfba59279dd6d46-nvidia-geforce-gtx-1080-ti/bias_act.cu -o bias_act.cuda.o \n/usr/include/c++/11/bits/std_function.h:435:145: error: parameter packs not expanded with ‘...’:\n  435 |         function(_Functor&& __f)\n      |                                                                                                                                                 ^ \n/usr/include/c++/11/bits/std_function.h:435:145: note:         ‘_ArgTypes’\n/usr/include/c++/11/bits/std_function.h:530:146: error: parameter packs not expanded with ‘...’:\n  530 |         operator=(_Functor&& __f)\n      |                                                                                                                                                  ^ \n/usr/include/c++/11/bits/std_function.h:530:146: note:         ‘_ArgTypes’\nninja: build stopped: subcommand failed.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/cpp_extension.py\u001b[0m in \u001b[0;36m_run_ninja_build\u001b[0;34m(build_directory, verbose, error_prefix)\u001b[0m\n\u001b[1;32m   1905\u001b[0m             \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1906\u001b[0;31m             env=env)\n\u001b[0m\u001b[1;32m   1907\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[0;32m--> 512\u001b[0;31m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['ninja', '-v']' returned non-zero exit status 1.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34368/955479470.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Seed {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_34368/1647447103.py\u001b[0m in \u001b[0;36mgenerate_image\u001b[0;34m(device, G, z, truncation_psi, noise_mode, class_idx)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation_psi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruncation_psi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m   img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(\\\n\u001b[1;32m     56\u001b[0m       torch.uint8)\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, c, truncation_psi, truncation_cutoff, update_emas, **synthesis_kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, c, truncation_psi, truncation_cutoff, update_emas)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m~/Py_workingdir/ml_learning/t81_558_deep_learning/stylegan3/torch_utils/ops/bias_act.py\u001b[0m in \u001b[0;36mbias_act\u001b[0;34m(x, b, dim, act, alpha, gain, clamp, impl)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ref'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_bias_act_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclamp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bias_act_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclamp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Py_workingdir/ml_learning/t81_558_deep_learning/stylegan3/torch_utils/ops/bias_act.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bias_act.h'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0msource_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mextra_cuda_cflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'--use_fast_math'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--allow-unsupported-compiler'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         )\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Py_workingdir/ml_learning/t81_558_deep_learning/stylegan3/torch_utils/custom_ops.py\u001b[0m in \u001b[0;36mget_plugin\u001b[0;34m(module_name, sources, headers, source_dir, **build_kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mcached_sources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_build_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             torch.utils.cpp_extension.load(name=module_name, build_directory=cached_build_dir,\n\u001b[0;32m--> 137\u001b[0;31m                 verbose=verbose_build, sources=cached_sources, **build_kwargs)\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpp_extension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose_build\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbuild_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/cpp_extension.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0mis_python_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mis_standalone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m         keep_intermediates=keep_intermediates)\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/cpp_extension.py\u001b[0m in \u001b[0;36m_jit_compile\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1516\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m                         \u001b[0mwith_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_cuda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m                         is_standalone=is_standalone)\n\u001b[0m\u001b[1;32m   1519\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m                 \u001b[0mbaton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/cpp_extension.py\u001b[0m in \u001b[0;36m_write_ninja_file_and_build_library\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_standalone)\u001b[0m\n\u001b[1;32m   1624\u001b[0m         \u001b[0mbuild_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1626\u001b[0;31m         error_prefix=f\"Error building extension '{name}'\")\n\u001b[0m\u001b[1;32m   1627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/cpp_extension.py\u001b[0m in \u001b[0;36m_run_ninja_build\u001b[0;34m(build_directory, verbose, error_prefix)\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'output'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\": {error.output.decode(*SUBPROCESS_DECODE_ARGS)}\"\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1916\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error building extension 'bias_act_plugin': [1/2] /home/adeng/miniconda3/envs/torch/bin/nvcc  -DTORCH_EXTENSION_NAME=bias_act_plugin -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/TH -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/THC -isystem /home/adeng/miniconda3/envs/torch/include -isystem /home/adeng/miniconda3/envs/torch/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 --compiler-options '-fPIC' --use_fast_math --allow-unsupported-compiler -std=c++14 -c /home/adeng/.cache/torch_extensions/py37_cu117/bias_act_plugin/3cb576a0039689487cfba59279dd6d46-nvidia-geforce-gtx-1080-ti/bias_act.cu -o bias_act.cuda.o \nFAILED: bias_act.cuda.o \n/home/adeng/miniconda3/envs/torch/bin/nvcc  -DTORCH_EXTENSION_NAME=bias_act_plugin -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/TH -isystem /home/adeng/miniconda3/envs/torch/lib/python3.7/site-packages/torch/include/THC -isystem /home/adeng/miniconda3/envs/torch/include -isystem /home/adeng/miniconda3/envs/torch/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 --compiler-options '-fPIC' --use_fast_math --allow-unsupported-compiler -std=c++14 -c /home/adeng/.cache/torch_extensions/py37_cu117/bias_act_plugin/3cb576a0039689487cfba59279dd6d46-nvidia-geforce-gtx-1080-ti/bias_act.cu -o bias_act.cuda.o \n/usr/include/c++/11/bits/std_function.h:435:145: error: parameter packs not expanded with ‘...’:\n  435 |         function(_Functor&& __f)\n      |                                                                                                                                                 ^ \n/usr/include/c++/11/bits/std_function.h:435:145: note:         ‘_ArgTypes’\n/usr/include/c++/11/bits/std_function.h:530:146: error: parameter packs not expanded with ‘...’:\n  530 |         operator=(_Functor&& __f)\n      |                                                                                                                                                  ^ \n/usr/include/c++/11/bits/std_function.h:530:146: note:         ‘_ArgTypes’\nninja: build stopped: subcommand failed.\n"
     ]
    }
   ],
   "source": [
    "# Choose your own starting and ending seed.\n",
    "SEED_FROM = 1000\n",
    "SEED_TO = 1003\n",
    "\n",
    "# Generate the images for the seeds.\n",
    "for i in range(SEED_FROM, SEED_TO):\n",
    "  print(f\"Seed {i}\")\n",
    "  z = seed2vec(G, i)\n",
    "  img = generate_image(device, G, z)\n",
    "  display_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N3Dcd2i52UT"
   },
   "source": [
    "## Examining the Latent Vector\n",
    "\n",
    "Figure 7.LVEC shows the effects of transforming the latent vector between two images. We accomplish this transformation by slowly moving one 512-value latent vector to another 512 vector. A high-dimension point between two latent vectors will appear similar to both of the two endpoint latent vectors. Images that have similar latent vectors will appear similar to each other.\n",
    "\n",
    "**Figure 7.LVEC: Transforming the Latent Vector**\n",
    "![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan_progression.jpg \"GAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "keozDTI_6EcA",
    "outputId": "01847966-da7b-4b1b-90b4-436e4a6f2eb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading networks from \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl\"...\n",
      "(1, 512)\n"
     ]
    }
   ],
   "source": [
    "def expand_seed(seeds, vector_size):\n",
    "  result = []\n",
    "\n",
    "  for seed in seeds:\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    result.append( rnd.randn(1, vector_size) ) \n",
    "  return result\n",
    "\n",
    "#URL = \"https://github.com/jeffheaton/pretrained-gan-fish/releases/\"\\\n",
    "#  \"download/1.0.0/fish-gan-2020-12-09.pkl\"\n",
    "#URL = \"https://github.com/jeffheaton/pretrained-merry-gan-mas/releases/\"\\\n",
    "#  \"download/v1/christmas-gan-2020-12-03.pkl\"\n",
    "#URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl\"\n",
    "URL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/\"\\\n",
    "  \"versions/1/files/stylegan3-r-ffhq-1024x1024.pkl\"\n",
    "\n",
    "print(f'Loading networks from \"{URL}\"...')\n",
    "device = torch.device('cuda')\n",
    "with dnnlib.util.open_url(URL) as f:\n",
    "    G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
    "\n",
    "vector_size = G.z_dim\n",
    "# range(8192,8300)\n",
    "seeds = expand_seed( [8192+1,8192+9], vector_size)\n",
    "#generate_images(Gs, seeds,truncation_psi=0.5)\n",
    "print(seeds[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fCn7OIM6caj"
   },
   "source": [
    "The following code will move between the provided seeds.  The constant STEPS specify how many frames there should be between each seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601,
     "referenced_widgets": [
      "d9233065c8874f208e35761bd03308ab",
      "a4b64ca6d6a64e5186b454c11a1e1bb4",
      "9af547f87ab545328c21aab4932ba24d",
      "450a2d55635844fb82854f485d263158",
      "2b5098dffdab4240960a906406a9a263",
      "ab7ec2e676ff45a8baf896af264e8e3c",
      "51609e17b679443e8f8c2521b0286ce7",
      "41284bfacd3c45dea994a0eaa6bc9109",
      "d7d2569612a848b98ab54bdb5311a42e",
      "d85084714db0412b99f98a41f6396a79",
      "56f60f38cefc47059328e68d9a9a9fea",
      "dac25f5c838147748a41822d603e8272",
      "86852e7232b6489f855d72098cb0e7f7",
      "55b4ed15a6a340c29bd16e68bd131026",
      "4d05f7a3e4344229b25a484dc80ae27c",
      "f906df8033b94579b714cd6c0514e38f",
      "9b1a086b7ae945e6bb47702458058fe4",
      "01435c0a85fc459abbc187ab5550d572",
      "b030dee27f2a4ab5a1fcaeb5c604b264",
      "a4ca70f6c1d44fa4bf99a8a7973b14bf",
      "a975055698c74b3e860f3007b556154b",
      "a9d276934b9d486aad21d24029606f3b"
     ]
    },
    "id": "VLI6T_8ZVqMJ",
    "outputId": "e8019984-9286-46c4-acf9-8328acab4544"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9233065c8874f208e35761bd03308ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 6624:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac25f5c838147748a41822d603e8272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 6618:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
      "  libavutil      55. 78.100 / 55. 78.100\n",
      "  libavcodec     57.107.100 / 57.107.100\n",
      "  libavformat    57. 83.100 / 57. 83.100\n",
      "  libavdevice    57. 10.100 / 57. 10.100\n",
      "  libavfilter     6.107.100 /  6.107.100\n",
      "  libavresample   3.  7.  0 /  3.  7.  0\n",
      "  libswscale      4.  8.100 /  4.  8.100\n",
      "  libswresample   2.  9.100 /  2.  9.100\n",
      "  libpostproc    54.  7.100 / 54.  7.100\n",
      "Input #0, image2, from '/content/results/frame-%d.png':\n",
      "  Duration: 00:00:08.00, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: png, rgb24(pc), 1024x1024, 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> mpeg4 (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, mp4, to 'movie.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf57.83.100\n",
      "    Stream #0:0: Video: mpeg4 (mp4v / 0x7634706D), yuv420p, 1024x1024, q=2-31, 200 kb/s, 30 fps, 15360 tbn, 30 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc57.107.100 mpeg4\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: -1\n",
      "frame=  200 fps= 43 q=31.0 Lsize=    1161kB time=00:00:06.63 bitrate=1433.4kbits/s speed=1.43x    \n",
      "video:1159kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.146784%\n"
     ]
    }
   ],
   "source": [
    "# HIDE OUTPUT\n",
    "# Choose your seeds to morph through and the number of steps to \n",
    "# take to get to each.\n",
    "\n",
    "SEEDS = [6624,6618,6616] # Better for faces\n",
    "#SEEDS = [1000,1003,1001] # Better for fish\n",
    "STEPS = 100\n",
    "\n",
    "# Remove any prior results\n",
    "!rm /content/results/* \n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "os.makedirs(\"./results/\", exist_ok=True)\n",
    "\n",
    "# Generate the images for the video.\n",
    "idx = 0\n",
    "for i in range(len(SEEDS)-1):\n",
    "  v1 = seed2vec(G, SEEDS[i])\n",
    "  v2 = seed2vec(G, SEEDS[i+1])\n",
    "\n",
    "  diff = v2 - v1\n",
    "  step = diff / STEPS\n",
    "  current = v1.copy()\n",
    "\n",
    "  for j in tqdm(range(STEPS), desc=f\"Seed {SEEDS[i]}\"):\n",
    "    current = current + step\n",
    "    img = generate_image(device, G, current)\n",
    "    img.save(f'./results/frame-{idx}.png')\n",
    "    idx+=1\n",
    " \n",
    "# Link the images into a video.\n",
    "!ffmpeg -r 30 -i /content/results/frame-%d.png -vcodec mpeg4 -y movie.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKWZQwJP7KDu"
   },
   "source": [
    "You can now download the generated video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "iQ5POSA77PFA",
    "outputId": "356b687f-26fc-477f-c392-5961378f0f9a"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_6d11c3f5-c512-4762-bc74-95967edc2d81\", \"movie.mp4\", 1188521)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download('movie.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beh4AmRnD91v"
   },
   "source": [
    "# Module 7 Assignment\n",
    "\n",
    "You can find the first assignment here: [assignment 7](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class7.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of t81_558_class_07_1_gan_intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01435c0a85fc459abbc187ab5550d572": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b5098dffdab4240960a906406a9a263": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41284bfacd3c45dea994a0eaa6bc9109": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "450a2d55635844fb82854f485d263158": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d85084714db0412b99f98a41f6396a79",
      "placeholder": "​",
      "style": "IPY_MODEL_56f60f38cefc47059328e68d9a9a9fea",
      "value": " 100/100 [00:52&lt;00:00,  1.75it/s]"
     }
    },
    "4d05f7a3e4344229b25a484dc80ae27c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a975055698c74b3e860f3007b556154b",
      "placeholder": "​",
      "style": "IPY_MODEL_a9d276934b9d486aad21d24029606f3b",
      "value": " 100/100 [00:57&lt;00:00,  1.67it/s]"
     }
    },
    "51609e17b679443e8f8c2521b0286ce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55b4ed15a6a340c29bd16e68bd131026": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b030dee27f2a4ab5a1fcaeb5c604b264",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a4ca70f6c1d44fa4bf99a8a7973b14bf",
      "value": 100
     }
    },
    "56f60f38cefc47059328e68d9a9a9fea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86852e7232b6489f855d72098cb0e7f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b1a086b7ae945e6bb47702458058fe4",
      "placeholder": "​",
      "style": "IPY_MODEL_01435c0a85fc459abbc187ab5550d572",
      "value": "Seed 6618: 100%"
     }
    },
    "9af547f87ab545328c21aab4932ba24d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41284bfacd3c45dea994a0eaa6bc9109",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d7d2569612a848b98ab54bdb5311a42e",
      "value": 100
     }
    },
    "9b1a086b7ae945e6bb47702458058fe4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4b64ca6d6a64e5186b454c11a1e1bb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab7ec2e676ff45a8baf896af264e8e3c",
      "placeholder": "​",
      "style": "IPY_MODEL_51609e17b679443e8f8c2521b0286ce7",
      "value": "Seed 6624: 100%"
     }
    },
    "a4ca70f6c1d44fa4bf99a8a7973b14bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a975055698c74b3e860f3007b556154b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9d276934b9d486aad21d24029606f3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab7ec2e676ff45a8baf896af264e8e3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b030dee27f2a4ab5a1fcaeb5c604b264": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7d2569612a848b98ab54bdb5311a42e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d85084714db0412b99f98a41f6396a79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9233065c8874f208e35761bd03308ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a4b64ca6d6a64e5186b454c11a1e1bb4",
       "IPY_MODEL_9af547f87ab545328c21aab4932ba24d",
       "IPY_MODEL_450a2d55635844fb82854f485d263158"
      ],
      "layout": "IPY_MODEL_2b5098dffdab4240960a906406a9a263"
     }
    },
    "dac25f5c838147748a41822d603e8272": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_86852e7232b6489f855d72098cb0e7f7",
       "IPY_MODEL_55b4ed15a6a340c29bd16e68bd131026",
       "IPY_MODEL_4d05f7a3e4344229b25a484dc80ae27c"
      ],
      "layout": "IPY_MODEL_f906df8033b94579b714cd6c0514e38f"
     }
    },
    "f906df8033b94579b714cd6c0514e38f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
